{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:34.919140Z",
     "end_time": "2023-04-21T23:22:37.037388Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "PWD = '../../..'\n",
    "sys.path.append(PWD)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import keras_tuner as kt\n",
    "\n",
    "from src.network_utils import count_params\n",
    "from src.cross_validator import KerasTunerCrossValidator\n",
    "from src.dataset import load_dataset_train_test, load_dataset_train_val, TIME_STEP\n",
    "from src.models import unet_builder as bare_model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'unet_dist'\n",
    "\n",
    "PLANE = 2\n",
    "CHANNEL = 11\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "DATASET_PATH = PWD + '/data/dataset/dataset.pkl'\n",
    "TRIALS_DIR = PWD + f'/data/model_selection/channel_{PLANE}_{CHANNEL}/tuner'\n",
    "CROSSVAL_DIR = PWD + f'/data/model_selection/channel_{PLANE}_{CHANNEL}/cross_val'\n",
    "\n",
    "LR = 0.01\n",
    "ES_MIN_DELTA = 0.01\n",
    "\n",
    "N_EPOCHS = 3000\n",
    "BATCH_SIZE = 2048\n",
    "MAX_TRIALS = 30\n",
    "EXECUTIONS_PER_TRIAL = 2\n",
    "\n",
    "TOP_N = 5\n",
    "CROSSVAL_N_CV = 5\n",
    "CROSSVAL_N_EXEC = 2\n",
    "LOSS_WEIGHT = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.039444Z",
     "end_time": "2023-04-21T23:22:37.053433Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "((22412, 24), (17929, 24), (4483, 24))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_base_train, _, y_base_train_original, _ = load_dataset_train_test(PWD, PLANE, CHANNEL)\n",
    "X_train, X_val, y_train_original, y_val_original = load_dataset_train_val(PWD, PLANE, CHANNEL)\n",
    "\n",
    "X_base_train.shape, X_train.shape, X_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.054433Z",
     "end_time": "2023-04-21T23:22:37.085471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "((22412, 24), (17929, 24), (4483, 24))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dist_kernel(y: int, n: int = 24):\n",
    "    x = np.arange(n) / TIME_STEP\n",
    "    return x - y\n",
    "\n",
    "\n",
    "y_base_train = np.array([dist_kernel(y) for y in y_base_train_original])\n",
    "y_train = np.array([dist_kernel(y) for y in y_train_original])\n",
    "y_val = np.array([dist_kernel(y) for y in y_val_original])\n",
    "\n",
    "y_base_train.shape, y_train.shape, y_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.086470Z",
     "end_time": "2023-04-21T23:22:37.183311Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def model_builder(hp: kt.HyperParameters) -> keras.Model:\n",
    "    hp_unet_depth = hp.Int(\"unet_depth\", min_value=0, max_value=3, step=1, default=2)\n",
    "    hp_n_conv_layers = hp.Int(\"n_conv_layers\", min_value=1, max_value=3, step=1)\n",
    "    hp_filters_mult = hp.Choice(\"conv_filters_mult\", values=[1, 2, 4, 8, 16], default=4)\n",
    "    hp_spatial_dropout = hp.Choice(\"conv_spatial_dropout\", values=[0.0, 0.1, 0.2])\n",
    "    hp_batch_normalization = hp.Boolean(\"batch_normalization\", default=False)\n",
    "    hp_input_batch_normalization = hp.Boolean(\"input_batch_normalization\", default=False)\n",
    "\n",
    "    model = bare_model_builder(hp_unet_depth, hp_n_conv_layers, hp_filters_mult, hp_spatial_dropout,\n",
    "                               hp_batch_normalization, hp_input_batch_normalization)\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(LR), loss_weights=LOSS_WEIGHT)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.183311Z",
     "end_time": "2023-04-21T23:22:37.211860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 24, 1)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 24, 32)       96          ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 12, 32)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 12, 64)       4160        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 6, 64)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 6, 128)       16512       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 12, 128)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 12, 64)       8256        ['up_sampling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12, 128)      0           ['conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 12, 64)       24640       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 24, 64)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 24, 32)       2080        ['up_sampling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 24, 64)       0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 24, 32)       6176        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 24, 1)        33          ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 24)           0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61,953\n",
      "Trainable params: 61,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_builder(kt.HyperParameters()).summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.197857Z",
     "end_time": "2023-04-21T23:22:37.765444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_callbacks = [\n",
    "    callbacks.EarlyStopping(patience=60, min_delta=ES_MIN_DELTA),\n",
    "    callbacks.ReduceLROnPlateau(monitor='loss', factor=0.9, patience=10)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T23:22:37.752194Z",
     "end_time": "2023-04-21T23:22:37.770447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian tuner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 Complete [00h 03m 55s]\n",
      "val_loss: 11.206226348876953\n",
      "\n",
      "Best val_loss So Far: 8.996604442596436\n",
      "Total elapsed time: 01h 23m 49s\n",
      "\n",
      "Search: Running Trial #28\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |3                 |unet_depth\n",
      "1                 |1                 |n_conv_layers\n",
      "16                |2                 |conv_filters_mult\n",
      "0.1               |0                 |conv_spatial_dropout\n",
      "True              |False             |batch_normalization\n",
      "True              |True              |input_batch_normalization\n",
      "\n",
      "Epoch 1/3000\n",
      "Epoch 2/3000\n",
      "Epoch 3/3000\n",
      "Epoch 4/3000\n",
      "Epoch 5/3000\n",
      "Epoch 6/3000\n",
      "Epoch 7/3000\n",
      "Epoch 8/3000\n",
      "Epoch 9/3000\n",
      "Epoch 10/3000\n",
      "Epoch 11/3000\n",
      "Epoch 12/3000\n",
      "Epoch 13/3000\n",
      "Epoch 14/3000\n",
      "Epoch 15/3000\n",
      "Epoch 16/3000\n",
      "Epoch 17/3000\n",
      "Epoch 18/3000\n",
      "Epoch 19/3000\n",
      "Epoch 20/3000\n",
      "Epoch 21/3000\n",
      "Epoch 22/3000\n",
      "Epoch 23/3000\n",
      "Epoch 24/3000\n",
      "Epoch 25/3000\n",
      "Epoch 26/3000\n",
      "Epoch 27/3000\n",
      "Epoch 28/3000\n",
      "Epoch 29/3000\n",
      "Epoch 30/3000\n",
      "Epoch 31/3000\n",
      "Epoch 32/3000\n",
      "Epoch 33/3000\n",
      "Epoch 34/3000\n",
      "Epoch 35/3000\n",
      "Epoch 36/3000\n",
      "Epoch 37/3000\n",
      "Epoch 38/3000\n",
      "Epoch 39/3000\n",
      "Epoch 40/3000\n",
      "Epoch 41/3000\n",
      "Epoch 42/3000\n",
      "Epoch 43/3000\n",
      "Epoch 44/3000\n",
      "Epoch 45/3000\n",
      "Epoch 46/3000\n",
      "Epoch 47/3000\n",
      "Epoch 48/3000\n",
      "Epoch 49/3000\n",
      "Epoch 50/3000\n",
      "Epoch 51/3000\n",
      "Epoch 52/3000\n",
      "Epoch 53/3000\n",
      "Epoch 54/3000\n",
      "Epoch 55/3000\n",
      "Epoch 56/3000\n",
      "Epoch 57/3000\n",
      "Epoch 58/3000\n",
      "Epoch 59/3000\n",
      "Epoch 60/3000\n",
      "Epoch 61/3000\n",
      "Epoch 62/3000\n",
      "Epoch 63/3000\n",
      "Epoch 64/3000\n",
      "Epoch 65/3000\n",
      "Epoch 66/3000\n",
      "Epoch 67/3000\n",
      "Epoch 68/3000\n",
      "Epoch 69/3000\n",
      "Epoch 70/3000\n",
      "Epoch 71/3000\n",
      "Epoch 72/3000\n",
      "Epoch 73/3000\n",
      "Epoch 74/3000\n",
      "Epoch 75/3000\n",
      "Epoch 76/3000\n",
      "Epoch 77/3000\n",
      "Epoch 78/3000\n",
      "Epoch 79/3000\n",
      "Epoch 80/3000\n",
      "Epoch 81/3000\n",
      "Epoch 82/3000\n",
      "Epoch 83/3000\n",
      "Epoch 84/3000\n",
      "Epoch 85/3000\n",
      "Epoch 86/3000\n",
      "Epoch 87/3000\n",
      "Epoch 88/3000\n",
      "Epoch 89/3000\n",
      "Epoch 90/3000\n",
      "Epoch 91/3000\n",
      "Epoch 92/3000\n",
      "Epoch 93/3000\n",
      "Epoch 94/3000\n",
      "Epoch 95/3000\n",
      "Epoch 96/3000\n",
      "Epoch 97/3000\n",
      "Epoch 98/3000\n",
      "Epoch 99/3000\n",
      "Epoch 100/3000\n",
      "Epoch 101/3000\n",
      "Epoch 102/3000\n",
      "Epoch 103/3000\n",
      "Epoch 104/3000\n",
      "Epoch 105/3000\n",
      "Epoch 106/3000\n",
      "Epoch 107/3000\n",
      "Epoch 108/3000\n",
      "Epoch 109/3000\n",
      "Epoch 110/3000\n",
      "Epoch 111/3000\n",
      "Epoch 112/3000\n",
      "Epoch 113/3000\n",
      "Epoch 114/3000\n",
      "Epoch 115/3000\n",
      "Epoch 116/3000\n",
      "Epoch 117/3000\n",
      "Epoch 118/3000\n",
      "Epoch 119/3000\n",
      "Epoch 120/3000\n",
      "Epoch 121/3000\n",
      "Epoch 122/3000\n",
      "Epoch 123/3000\n",
      "Epoch 124/3000\n",
      "Epoch 125/3000\n",
      "Epoch 126/3000\n",
      "Epoch 127/3000\n",
      "Epoch 128/3000\n",
      "Epoch 129/3000\n",
      "Epoch 130/3000\n",
      "Epoch 131/3000\n",
      "Epoch 132/3000\n",
      "Epoch 133/3000\n",
      "Epoch 134/3000\n",
      "Epoch 135/3000\n",
      "Epoch 136/3000\n",
      "Epoch 137/3000\n",
      "Epoch 138/3000\n",
      "Epoch 139/3000\n",
      "Epoch 140/3000\n",
      "Epoch 141/3000\n",
      "Epoch 142/3000\n",
      "Epoch 143/3000\n",
      "Epoch 144/3000\n",
      "Epoch 145/3000\n",
      "Epoch 146/3000\n",
      "Epoch 147/3000\n",
      "Epoch 148/3000\n",
      "Epoch 149/3000\n",
      "Epoch 150/3000\n",
      "Epoch 151/3000\n",
      "Epoch 152/3000\n",
      "Epoch 153/3000\n",
      "Epoch 154/3000\n",
      "Epoch 155/3000\n",
      "Epoch 156/3000\n",
      "Epoch 157/3000\n",
      "Epoch 158/3000\n",
      "Epoch 159/3000\n",
      "Epoch 160/3000\n",
      "Epoch 161/3000\n",
      "Epoch 162/3000\n",
      "Epoch 163/3000\n",
      "Epoch 164/3000\n",
      "Epoch 165/3000\n",
      "Epoch 166/3000\n",
      "Epoch 167/3000\n",
      "Epoch 168/3000\n",
      "Epoch 169/3000\n",
      "Epoch 170/3000\n",
      "Epoch 171/3000\n",
      "Epoch 172/3000\n",
      "Epoch 173/3000\n",
      "Epoch 174/3000\n",
      "Epoch 175/3000\n",
      "Epoch 176/3000\n",
      "Epoch 177/3000\n",
      "Epoch 178/3000\n",
      "Epoch 179/3000\n",
      "Epoch 180/3000\n",
      "Epoch 181/3000\n",
      "Epoch 182/3000\n",
      "Epoch 183/3000\n",
      "Epoch 184/3000\n",
      "Epoch 185/3000\n",
      "Epoch 186/3000\n",
      "Epoch 187/3000\n",
      "Epoch 188/3000\n",
      "Epoch 189/3000\n",
      "Epoch 190/3000\n",
      "Epoch 191/3000\n",
      "Epoch 192/3000\n",
      "Epoch 193/3000\n",
      "Epoch 194/3000\n",
      "Epoch 195/3000\n",
      "Epoch 196/3000\n",
      "Epoch 197/3000\n",
      "Epoch 198/3000\n",
      "Epoch 199/3000\n",
      "Epoch 200/3000\n",
      "Epoch 201/3000\n",
      "Epoch 202/3000\n",
      "Epoch 203/3000\n",
      "Epoch 204/3000\n",
      "Epoch 205/3000\n",
      "Epoch 206/3000\n",
      "Epoch 207/3000\n",
      "Epoch 208/3000\n",
      "Epoch 209/3000\n",
      "Epoch 210/3000\n",
      "Epoch 211/3000\n",
      "Epoch 212/3000\n",
      "Epoch 213/3000\n",
      "Epoch 214/3000\n",
      "Epoch 215/3000\n",
      "Epoch 216/3000\n",
      "Epoch 217/3000\n",
      "Epoch 218/3000\n",
      "Epoch 219/3000\n",
      "Epoch 220/3000\n",
      "Epoch 221/3000\n",
      "Epoch 222/3000\n",
      "Epoch 223/3000\n",
      "Epoch 224/3000\n",
      "Epoch 225/3000\n",
      "Epoch 226/3000\n",
      "Epoch 227/3000\n",
      "Epoch 228/3000\n",
      "Epoch 229/3000\n",
      "Epoch 230/3000\n",
      "Epoch 231/3000\n",
      "Epoch 232/3000\n",
      "Epoch 233/3000\n",
      "Epoch 234/3000\n",
      "Epoch 235/3000\n",
      "Epoch 236/3000\n",
      "Epoch 237/3000\n",
      "Epoch 238/3000\n",
      "Epoch 239/3000\n",
      "Epoch 240/3000\n",
      "Epoch 241/3000\n",
      "Epoch 242/3000\n",
      "Epoch 243/3000\n",
      "Epoch 244/3000\n",
      "Epoch 245/3000\n",
      "Epoch 246/3000\n",
      "Epoch 247/3000\n",
      "Epoch 248/3000\n",
      "Epoch 249/3000\n",
      "Epoch 250/3000\n",
      "Epoch 251/3000\n",
      "Epoch 252/3000\n",
      "Epoch 253/3000\n",
      "Epoch 254/3000\n",
      "Epoch 255/3000\n",
      "Epoch 256/3000\n",
      "Epoch 257/3000\n",
      "Epoch 258/3000\n",
      "Epoch 259/3000\n",
      "Epoch 260/3000\n",
      "Epoch 261/3000\n",
      "Epoch 262/3000\n",
      "Epoch 263/3000\n",
      "Epoch 264/3000\n",
      "Epoch 265/3000\n",
      "Epoch 266/3000\n",
      "Epoch 267/3000\n",
      "Epoch 268/3000\n",
      "Epoch 269/3000\n",
      "Epoch 270/3000\n",
      "Epoch 271/3000\n",
      "Epoch 272/3000\n",
      "Epoch 273/3000\n",
      "Epoch 274/3000\n",
      "Epoch 275/3000\n",
      "Epoch 276/3000\n",
      "Epoch 277/3000\n",
      "Epoch 278/3000\n",
      "Epoch 279/3000\n",
      "Epoch 280/3000\n",
      "Epoch 281/3000\n",
      "Epoch 282/3000\n",
      "Epoch 283/3000\n",
      "Epoch 284/3000\n",
      "Epoch 285/3000\n",
      "Epoch 286/3000\n",
      "Epoch 287/3000\n",
      "Epoch 288/3000\n",
      "Epoch 289/3000\n",
      "Epoch 290/3000\n",
      "Epoch 291/3000\n",
      "Epoch 292/3000\n",
      "Epoch 293/3000\n",
      "Epoch 294/3000\n",
      "Epoch 295/3000\n",
      "Epoch 296/3000\n",
      "Epoch 297/3000\n",
      "Epoch 298/3000\n",
      "Epoch 299/3000\n",
      "Epoch 300/3000\n",
      "Epoch 301/3000\n",
      "Epoch 302/3000\n",
      "Epoch 303/3000\n",
      "Epoch 304/3000\n",
      "Epoch 305/3000\n",
      "Epoch 306/3000\n",
      "Epoch 307/3000\n",
      "Epoch 308/3000\n",
      "Epoch 309/3000\n",
      "Epoch 310/3000\n",
      "Epoch 311/3000\n",
      "Epoch 312/3000\n",
      "Epoch 313/3000\n",
      "Epoch 314/3000\n",
      "Epoch 315/3000\n",
      "Epoch 316/3000\n",
      "Epoch 317/3000\n",
      "Epoch 318/3000\n",
      "Epoch 319/3000\n",
      "Epoch 320/3000\n",
      "Epoch 321/3000\n",
      "Epoch 322/3000\n",
      "Epoch 323/3000\n",
      "Epoch 324/3000\n",
      "Epoch 325/3000\n",
      "Epoch 326/3000\n",
      "Epoch 327/3000\n",
      "Epoch 328/3000\n",
      "Epoch 329/3000\n",
      "Epoch 330/3000\n",
      "Epoch 331/3000\n",
      "Epoch 332/3000\n",
      "Epoch 333/3000\n",
      "Epoch 334/3000\n",
      "Epoch 335/3000\n",
      "Epoch 336/3000\n",
      "Epoch 337/3000\n",
      "Epoch 338/3000\n",
      "Epoch 339/3000\n",
      "Epoch 340/3000\n",
      "Epoch 341/3000\n",
      "Epoch 342/3000\n",
      "Epoch 343/3000\n",
      "Epoch 344/3000\n",
      "Epoch 345/3000\n",
      "Epoch 346/3000\n",
      "Epoch 347/3000\n",
      "Epoch 348/3000\n",
      "Epoch 349/3000\n",
      "Epoch 350/3000\n",
      "Epoch 351/3000\n",
      "Epoch 352/3000\n",
      "Epoch 353/3000\n",
      "Epoch 354/3000\n",
      "Epoch 355/3000\n",
      "Epoch 356/3000\n",
      "Epoch 357/3000\n",
      "Epoch 358/3000\n",
      "Epoch 359/3000\n",
      "Epoch 360/3000\n",
      "Epoch 361/3000\n",
      "Epoch 362/3000\n",
      "Epoch 363/3000\n",
      "Epoch 364/3000\n",
      "Epoch 365/3000\n",
      "Epoch 366/3000\n",
      "Epoch 367/3000\n",
      "Epoch 368/3000\n",
      "Epoch 369/3000\n",
      "Epoch 370/3000\n",
      "Epoch 371/3000\n",
      "Epoch 372/3000\n",
      "Epoch 373/3000\n",
      "Epoch 374/3000\n",
      "Epoch 375/3000\n",
      "Epoch 376/3000\n",
      "Epoch 377/3000\n",
      "Epoch 378/3000\n",
      "Epoch 379/3000\n",
      "Epoch 380/3000\n",
      "Epoch 381/3000\n",
      "Epoch 382/3000\n",
      "Epoch 383/3000\n",
      "Epoch 384/3000\n",
      "Epoch 385/3000\n",
      "Epoch 386/3000\n",
      "Epoch 387/3000\n",
      "Epoch 388/3000\n",
      "Epoch 389/3000\n",
      "Epoch 390/3000\n",
      "Epoch 391/3000\n",
      "Epoch 392/3000\n",
      "Epoch 393/3000\n",
      "Epoch 394/3000\n",
      "Epoch 395/3000\n",
      "Epoch 396/3000\n",
      "Epoch 397/3000\n",
      "Epoch 398/3000\n",
      "Epoch 399/3000\n",
      "Epoch 400/3000\n",
      "Epoch 401/3000\n",
      "Epoch 402/3000\n",
      "Epoch 403/3000\n",
      "Epoch 404/3000\n",
      "Epoch 405/3000\n",
      "Epoch 406/3000\n",
      "Epoch 407/3000\n",
      "Epoch 408/3000\n",
      "Epoch 409/3000\n",
      "Epoch 410/3000\n",
      "Epoch 411/3000\n",
      "Epoch 412/3000\n",
      "Epoch 413/3000\n",
      "Epoch 414/3000\n",
      "Epoch 415/3000\n",
      "Epoch 416/3000\n",
      "Epoch 417/3000\n",
      "Epoch 418/3000\n",
      "Epoch 419/3000\n",
      "Epoch 420/3000\n",
      "Epoch 421/3000\n",
      "Epoch 422/3000\n",
      "Epoch 423/3000\n",
      "Epoch 424/3000\n",
      "Epoch 425/3000\n",
      "Epoch 426/3000\n",
      "Epoch 427/3000\n",
      "Epoch 428/3000\n",
      "Epoch 429/3000\n",
      "Epoch 430/3000\n",
      "Epoch 431/3000\n",
      "Epoch 432/3000\n",
      "Epoch 433/3000\n",
      "Epoch 434/3000\n",
      "Epoch 435/3000\n",
      "Epoch 436/3000\n",
      "Epoch 437/3000\n",
      "Epoch 438/3000\n",
      "Epoch 439/3000\n",
      "Epoch 440/3000\n",
      "Epoch 441/3000\n",
      "Epoch 442/3000\n",
      "Epoch 443/3000\n",
      "Epoch 444/3000\n",
      "Epoch 445/3000\n",
      "Epoch 446/3000\n",
      "Epoch 447/3000\n",
      "Epoch 448/3000\n",
      "Epoch 449/3000\n",
      "Epoch 450/3000\n",
      "Epoch 451/3000\n",
      "Epoch 452/3000\n",
      "Epoch 453/3000\n",
      "Epoch 454/3000\n",
      "Epoch 455/3000\n",
      "Epoch 456/3000\n",
      "Epoch 457/3000\n",
      "Epoch 458/3000\n",
      "Epoch 459/3000\n",
      "Epoch 460/3000\n",
      "Epoch 461/3000\n",
      "Epoch 462/3000\n",
      "Epoch 463/3000\n",
      "Epoch 464/3000\n",
      "Epoch 465/3000\n",
      "Epoch 466/3000\n",
      "Epoch 467/3000\n",
      "Epoch 468/3000\n",
      "Epoch 469/3000\n",
      "Epoch 470/3000\n",
      "Epoch 471/3000\n",
      "Epoch 472/3000\n",
      "Epoch 473/3000\n",
      "Epoch 474/3000\n",
      "Epoch 475/3000\n",
      "Epoch 476/3000\n",
      "Epoch 477/3000\n",
      "Epoch 478/3000\n",
      "Epoch 479/3000\n",
      "Epoch 480/3000\n",
      "Epoch 481/3000\n",
      "Epoch 482/3000\n",
      "Epoch 483/3000\n",
      "Epoch 484/3000\n",
      "Epoch 485/3000\n",
      "Epoch 486/3000\n",
      "Epoch 487/3000\n",
      "Epoch 488/3000\n",
      "Epoch 489/3000\n",
      "Epoch 490/3000\n",
      "Epoch 491/3000\n",
      "Epoch 492/3000\n",
      "Epoch 493/3000\n",
      "Epoch 494/3000\n",
      "Epoch 495/3000\n",
      "Epoch 496/3000\n",
      "Epoch 497/3000\n",
      "Epoch 498/3000\n",
      "Epoch 499/3000\n",
      "Epoch 500/3000\n",
      "Epoch 501/3000\n",
      "Epoch 502/3000\n",
      "Epoch 503/3000\n",
      "Epoch 504/3000\n",
      "Epoch 505/3000\n",
      "Epoch 506/3000\n",
      "Epoch 507/3000\n",
      "Epoch 508/3000\n",
      "Epoch 509/3000\n",
      "Epoch 510/3000\n",
      "Epoch 511/3000\n",
      "Epoch 512/3000\n",
      "Epoch 513/3000\n",
      "Epoch 514/3000\n",
      "Epoch 515/3000\n",
      "Epoch 516/3000\n",
      "Epoch 517/3000\n",
      "Epoch 518/3000\n",
      "Epoch 519/3000\n",
      "Epoch 520/3000\n",
      "Epoch 521/3000\n",
      "Epoch 522/3000\n",
      "Epoch 523/3000\n",
      "Epoch 524/3000\n",
      "Epoch 525/3000\n",
      "Epoch 526/3000\n",
      "Epoch 527/3000\n",
      "Epoch 528/3000\n",
      "Epoch 529/3000\n",
      "Epoch 530/3000\n",
      "Epoch 531/3000\n",
      "Epoch 532/3000\n",
      "Epoch 533/3000\n",
      "Epoch 534/3000\n",
      "Epoch 535/3000\n",
      "Epoch 536/3000\n",
      "Epoch 537/3000\n",
      "Epoch 538/3000\n",
      "Epoch 539/3000\n",
      "Epoch 540/3000\n",
      "Epoch 541/3000\n",
      "Epoch 542/3000\n",
      "Epoch 543/3000\n",
      "Epoch 544/3000\n",
      "Epoch 545/3000\n",
      "Epoch 546/3000\n",
      "Epoch 547/3000\n",
      "Epoch 548/3000\n",
      "Epoch 549/3000\n",
      "Epoch 550/3000\n",
      "Epoch 551/3000\n",
      "Epoch 552/3000\n",
      "Epoch 553/3000\n",
      "Epoch 554/3000\n",
      "Epoch 555/3000\n",
      "Epoch 556/3000\n",
      "Epoch 557/3000\n",
      "Epoch 558/3000\n",
      "Epoch 559/3000\n",
      "Epoch 560/3000\n",
      "Epoch 561/3000\n",
      "Epoch 562/3000\n",
      "Epoch 563/3000\n",
      "Epoch 564/3000\n",
      "Epoch 565/3000\n",
      "Epoch 566/3000\n",
      "Epoch 567/3000\n",
      "Epoch 568/3000\n",
      "Epoch 569/3000\n",
      "Epoch 570/3000\n",
      "Epoch 571/3000\n",
      "Epoch 572/3000\n",
      "Epoch 573/3000\n",
      "Epoch 574/3000\n",
      "Epoch 575/3000\n",
      "Epoch 576/3000\n",
      "Epoch 577/3000\n",
      "Epoch 578/3000\n",
      "Epoch 579/3000\n",
      "Epoch 580/3000\n",
      "Epoch 581/3000\n",
      "Epoch 582/3000\n",
      "Epoch 583/3000\n",
      "Epoch 584/3000\n",
      "Epoch 585/3000\n",
      "Epoch 586/3000\n",
      "Epoch 587/3000\n",
      "Epoch 588/3000\n",
      "Epoch 589/3000\n",
      "Epoch 590/3000\n",
      "Epoch 591/3000\n",
      "Epoch 592/3000\n",
      "Epoch 593/3000\n",
      "Epoch 594/3000\n",
      "Epoch 595/3000\n",
      "Epoch 596/3000\n",
      "Epoch 597/3000\n",
      "Epoch 598/3000\n",
      "Epoch 599/3000\n",
      "Epoch 600/3000\n",
      "Epoch 601/3000\n",
      "Epoch 602/3000\n",
      "Epoch 603/3000\n",
      "Epoch 604/3000\n",
      "Epoch 605/3000\n",
      "Epoch 606/3000\n",
      "Epoch 607/3000\n",
      "Epoch 608/3000\n",
      "Epoch 609/3000\n",
      "Epoch 610/3000\n",
      "Epoch 611/3000\n",
      "Epoch 612/3000\n",
      "Epoch 613/3000\n",
      "Epoch 614/3000\n",
      "Epoch 615/3000\n",
      "Epoch 616/3000\n",
      "Epoch 617/3000\n",
      "Epoch 618/3000\n",
      "Epoch 619/3000\n",
      "Epoch 620/3000\n",
      "Epoch 621/3000\n",
      "Epoch 622/3000\n",
      "Epoch 623/3000\n",
      "Epoch 624/3000\n",
      "Epoch 625/3000\n",
      "Epoch 626/3000\n",
      "Epoch 627/3000\n",
      "Epoch 628/3000\n",
      "Epoch 629/3000\n",
      "Epoch 630/3000\n",
      "Epoch 631/3000\n",
      "Epoch 632/3000\n",
      "Epoch 633/3000\n",
      "Epoch 634/3000\n",
      "Epoch 635/3000\n",
      "Epoch 636/3000\n",
      "Epoch 637/3000\n",
      "Epoch 638/3000\n",
      "Epoch 639/3000\n",
      "Epoch 640/3000\n",
      "Epoch 1/3000\n",
      "Epoch 2/3000\n",
      "Epoch 3/3000\n",
      "Epoch 4/3000\n",
      "Epoch 5/3000\n",
      "Epoch 6/3000\n",
      "Epoch 7/3000\n",
      "Epoch 8/3000\n",
      "Epoch 9/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m bayesian_tuner \u001B[38;5;241m=\u001B[39m kt\u001B[38;5;241m.\u001B[39mBayesianOptimization(model_builder, objective\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, executions_per_trial\u001B[38;5;241m=\u001B[39mEXECUTIONS_PER_TRIAL,\n\u001B[0;32m      2\u001B[0m                                          max_trials\u001B[38;5;241m=\u001B[39mMAX_TRIALS, directory\u001B[38;5;241m=\u001B[39mTRIALS_DIR, project_name\u001B[38;5;241m=\u001B[39mPROJECT_NAME,\n\u001B[0;32m      3\u001B[0m                                          overwrite\u001B[38;5;241m=\u001B[39mOVERWRITE)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mbayesian_tuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_callbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:226\u001B[0m, in \u001B[0;36mBaseTuner.search\u001B[1;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_begin(trial)\n\u001B[1;32m--> 226\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_run_and_update_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_trial_end(trial)\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_search_end()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:266\u001B[0m, in \u001B[0;36mBaseTuner._try_run_and_update_trial\u001B[1;34m(self, trial, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_run_and_update_trial\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 266\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_and_update_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    267\u001B[0m         trial\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m trial_module\u001B[38;5;241m.\u001B[39mTrialStatus\u001B[38;5;241m.\u001B[39mCOMPLETED\n\u001B[0;32m    268\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:231\u001B[0m, in \u001B[0;36mBaseTuner._run_and_update_trial\u001B[1;34m(self, trial, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_and_update_trial\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[1;32m--> 231\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_trial(trial, \u001B[38;5;241m*\u001B[39mfit_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mget_trial(trial\u001B[38;5;241m.\u001B[39mtrial_id)\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mexists(\n\u001B[0;32m    233\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective\u001B[38;5;241m.\u001B[39mname\n\u001B[0;32m    234\u001B[0m     ):\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         \u001B[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001B[39;00m\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;66;03m# use case. No further action needed in this case.\u001B[39;00m\n\u001B[0;32m    238\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    239\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe use case of calling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    246\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    247\u001B[0m         )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:287\u001B[0m, in \u001B[0;36mTuner.run_trial\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    285\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(model_checkpoint)\n\u001B[0;32m    286\u001B[0m     copied_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m callbacks\n\u001B[1;32m--> 287\u001B[0m     obj_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_and_fit_model(trial, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcopied_kwargs)\n\u001B[0;32m    289\u001B[0m     histories\u001B[38;5;241m.\u001B[39mappend(obj_value)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m histories\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:214\u001B[0m, in \u001B[0;36mTuner._build_and_fit_model\u001B[1;34m(self, trial, *args, **kwargs)\u001B[0m\n\u001B[0;32m    212\u001B[0m hp \u001B[38;5;241m=\u001B[39m trial\u001B[38;5;241m.\u001B[39mhyperparameters\n\u001B[0;32m    213\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_build(hp)\n\u001B[1;32m--> 214\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mfit(hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    215\u001B[0m tuner_utils\u001B[38;5;241m.\u001B[39mvalidate_trial_results(\n\u001B[0;32m    216\u001B[0m     results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moracle\u001B[38;5;241m.\u001B[39mobjective, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHyperModel.fit()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    217\u001B[0m )\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:144\u001B[0m, in \u001B[0;36mHyperModel.fit\u001B[1;34m(self, hp, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, hp, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Train the model.\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \n\u001B[0;32m    123\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m        If return a float, it should be the `objective` value.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\engine\\training.py:1570\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1568\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs\n\u001B[0;32m   1569\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1570\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1572\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\callbacks.py:470\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \n\u001B[0;32m    465\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 470\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\callbacks.py:317\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 317\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\callbacks.py:340\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    337\u001B[0m     batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    343\u001B[0m     end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\callbacks.py:385\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    383\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 385\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_logs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_batch_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    387\u001B[0m     hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\callbacks.py:292\u001B[0m, in \u001B[0;36mCallbackList._process_logs\u001B[1;34m(self, logs, is_batch_hook)\u001B[0m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_batch_hook \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_hooks_support_tf_logs:\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logs\n\u001B[1;32m--> 292\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m--> 635\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 628\u001B[0m         t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;66;03m# as-is.\u001B[39;00m\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \n\u001B[0;32m   1136\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cern-ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1122\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1124\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "bayesian_tuner = kt.BayesianOptimization(model_builder, objective='val_loss', executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
    "                                         max_trials=MAX_TRIALS, directory=TRIALS_DIR, project_name=PROJECT_NAME,\n",
    "                                         overwrite=OVERWRITE)\n",
    "\n",
    "bayesian_tuner.search(X_train, y_train, validation_data=[X_val, y_val], epochs=N_EPOCHS, callbacks=model_callbacks,\n",
    "                      batch_size=BATCH_SIZE, verbose=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T18:08:14.707719Z",
     "end_time": "2023-04-10T21:07:46.886377Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Model 0 ==========\n",
      "{'unet_depth': 3, 'n_conv_layers': 1, 'conv_filters_mult': 2, 'conv_spatial_dropout': 0.0, 'batch_normalization': False, 'input_batch_normalization': True}\n",
      "Number of parameters: 65029\n",
      "========== Model 1 ==========\n",
      "{'unet_depth': 3, 'n_conv_layers': 2, 'conv_filters_mult': 8, 'conv_spatial_dropout': 0.0, 'batch_normalization': True, 'input_batch_normalization': True}\n",
      "Number of parameters: 1472389\n",
      "========== Model 2 ==========\n",
      "{'unet_depth': 3, 'n_conv_layers': 1, 'conv_filters_mult': 4, 'conv_spatial_dropout': 0.1, 'batch_normalization': True, 'input_batch_normalization': True}\n",
      "Number of parameters: 260869\n",
      "========== Model 3 ==========\n",
      "{'unet_depth': 2, 'n_conv_layers': 2, 'conv_filters_mult': 2, 'conv_spatial_dropout': 0.0, 'batch_normalization': True, 'input_batch_normalization': True}\n",
      "Number of parameters: 22885\n",
      "========== Model 4 ==========\n",
      "{'unet_depth': 3, 'n_conv_layers': 2, 'conv_filters_mult': 2, 'conv_spatial_dropout': 0.2, 'batch_normalization': True, 'input_batch_normalization': True}\n",
      "Number of parameters: 93925\n"
     ]
    }
   ],
   "source": [
    "for i, hyperparameters in enumerate(bayesian_tuner.get_best_hyperparameters(TOP_N)):\n",
    "    print(f'========== Model {i} ==========')\n",
    "    print(hyperparameters.get_config()['values'])\n",
    "    model_tmp = model_builder(hyperparameters)\n",
    "    print('Number of parameters:', count_params(model_tmp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T00:53:30.536461Z",
     "end_time": "2023-04-22T00:53:31.240190Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross-validation for top 5 models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_validator = KerasTunerCrossValidator(bayesian_tuner, X_base_train, y_base_train, model_builder,\n",
    "                                           directory=CROSSVAL_DIR, project_name=PROJECT_NAME,\n",
    "                                           n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, n_top=TOP_N,\n",
    "                                           es_min_delta=ES_MIN_DELTA, n_cv=CROSSVAL_N_CV, n_executions=CROSSVAL_N_EXEC,\n",
    "                                           overwrite=OVERWRITE)\n",
    "model_scores = cross_validator()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T21:07:47.438044Z",
     "end_time": "2023-04-10T21:41:21.130042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_scores = [f\"{np.mean(scores):0.2f}\" for scores in model_scores.values()]\n",
    "std_scores = [f\"{np.std(scores):0.2f}\" for scores in model_scores.values()]\n",
    "n_params = [count_params(model_builder(hyperparameters)) for hyperparameters in\n",
    "            bayesian_tuner.get_best_hyperparameters(TOP_N)]\n",
    "\n",
    "df = pd.DataFrame({'mean': mean_scores, 'std': std_scores, 'n_params': n_params}, index=model_scores.keys())\n",
    "df.index.name = 'Model'\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T21:41:21.131042Z",
     "end_time": "2023-04-10T21:41:21.650310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T21:41:21.651310Z",
     "end_time": "2023-04-10T21:41:21.669622Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
